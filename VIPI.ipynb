{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VIPI.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"noR4t-jEjobD","colab_type":"code","colab":{}},"source":["import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","#Creates the world for this example\n","class grid_world():\n","  def __init__(self,n):\n","    self.size=n\n","  def build_world(self,n):\n","    world=np.zeros((n,n))\n","    world[0,0]=1\n","    world[0,n-1]=10\n","    return world\n","  #Set of actions available, since a movement out of bounds is caught later all movements are possible.\n","  def get_actions(self,i,j,n):\n","    moves=[]\n","    if(i!=0):\n","      moves.append((-1,0))\n","    if(i!=self.size-1):\n","      moves.append((1,0))\n","    if(j!=0):\n","      moves.append((0,-1))      \n","    if(j!=self.size-1):\n","      moves.append((0,1))      \n","    return moves\n","\n","  def get_states(self,n):\n","    return np.meshgrid(range(n),range(n))\n","\n","  #Builds the matrix of state action pairs\n","  def transition_actions(self,n):\n","    transitions=[[[] for i in range(n)] for j in range(n)]\n","    for i in range(n):\n","      for j in range(n):\n","        transitions[i][j]=get_actions(i,j,n)\n","    return(transitions)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Uvf8sAFlZMl","colab_type":"code","colab":{}},"source":["def value_iteration(rewards,possible_actions,n,p_action, lbda):\n","\t#Initializations of V and theta the convergence condition\n","\tV=np.zeros((n,n))\n","\tV_new=np.zeros((n,n))\n","\tconverged=0\n","\ttheta=.0001\n","\tcount=0\n","\n","\twhile(not converged):\n","\t\tcount=count+1\n","\t\t#Loop over all states skipping the top left and the top right\n","\t\tfor i in range(n):\n","\t\t\tfor j in range(n):\n","\t\t\t\tif(i==0 and j==0):\n","\t\t\t\t\tcontinue\n","\t\t\t\tif(i==0 and j==n-1):\n","\t\t\t\t\tcontinue\n","\t\t\t\taction_rewards=[]\n","\n","\t\t\t\t#Loop over actions to get the value if that action was executed and then sum them according to the probability of that state happening for a given action\n","\t\t\t\t#The max min part bounds the actions into the grid\n","\t\t\t\tfor action in possible_actions[i][j]:\n","\n","        final_action_rewards=[]\n","\t\t\t\tfor ii in range(len(action_rewards)):\n","\n","          #Update Values matrix\n","          \n","\t\t\t\tV_new[i,j]=max(final_action_rewards)\n","\t\tconverged=np.sum(abs(V-V_new))<theta\n","\t\tV=V_new.copy()\n","\treturn(V,bottom_left,bottom_right)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"39owMShKicSn","colab_type":"code","colab":{}},"source":["def eval_policy(policy,rewards,possible_actions,n,p_action,lbda,num_iterations):\n","\tV=np.zeros((n,n))\n","\tV_new=np.zeros((n,n))\n","\tconverged=0\n","\ttheta=.1\n","\t#Evaluates the given policy: if num_iterations is greater than one it does modified policy otherwise it iterates until convergence \n","\tif(num_iterations<0):\n","\t\twhile(not converged):\n","\t\t\tfor i in range(n):\n","\t\t\t\tfor j in range(n):\n","\t\t\t\t\tif(i==0 and j==0):\n","\t\t\t\t\t\tcontinue\n","\t\t\t\t\tif(i==0 and j==n-1):\n","\t\t\t\t\t\tcontinue\n","\t\t\t\t\taction_rewards=[]\n","\t\t\t\t\taction_probs=[]\n","\t\t\t\t\t#Loops over states and calculates the value of taking an action, calculates the policy's chance of taking that action then combines the two to get an evaluation\n","\t\t\t\t\tfor action in policy[i][j][0]:\n","            #Fill in from psuedo code\n","\t\t\t\t\tfor action_prob in policy[i][j][1]:\n","\t\t\t\t\t\taction_probs.append(action_prob)\n","\t\t\t\t\tfinal_action_rewards=[]\n","\t\t\t\t\tfor ii in range(len(action_rewards)):\n","            #Fill in from psuedo code\n","\t\t\t\t\tV_new[i,j]=max(final_action_rewards)\n","\t\t\tconverged=np.sum(abs(V-V_new))<theta\n","\t\t\tV=V_new.copy()\n","\t\n","\telse:\n","\t\tfor k in range(num_iterations):\n","\t\t\tfor i in range(n):\n","\t\t\t\tfor j in range(n):\n","\t\t\t\t\tif(i==0 and j==0):\n","\t\t\t\t\t\tcontinue\n","\t\t\t\t\tif(i==0 and j==n-1):\n","\t\t\t\t\t\tcontinue\n","\t\t\t\t\taction_rewards=[]\n","\t\t\t\t\taction_probs=[]\n","\t\t\t\t\tfor action in policy[i][j][0]:\n","\n","          for action_prob in policy[i][j][1]:\n","\n","          final_action_rewards=[]\n","\t\t\t\t\tfor ii in range(len(action_rewards)):\n","\n","          V_new[i,j]=max(final_action_rewards)\n","\t\t\tV=V_new.copy()\n","\treturn V\t\n","\n","\n","#Initializes a policy based on a set of actions\n","def initialize_policy(n,possible_actions):\n","\tpolicy=[[[[]for i in range(2)] for j in range(n)] for k in range(n)] \n","\tfor i in range(n):\n","\t\tfor j in range(n):\n","\t\t\tstate_policy=np.zeros((len(possible_actions[i][j])))\n","\t\t\tstate_policy[np.random.randint(0,len(possible_actions[i][j]))]=1\n","\t\t\tpolicy[i][j][1]=state_policy\n","\t\t\tpolicy[i][j][0]=possible_actions[i][j]\t\t\t\n","\treturn policy\n","\n","def policy_iteration(rewards,possible_actions,n,p_action, lbda):\n","\tV=np.zeros((n,n))\n","\tpi=np.zeros((n,n))\n","\tpolicy=initialize_policy(n,possible_actions)\n","\tV_new=np.zeros((n,n))\n","\tconverged=0\n","\tcount=0\n","\tbottom_left=[]\n","\tbottom_right=[]\n","\tbottom_left.append(0)\n","\tbottom_right.append(0)\n","\twhile(not converged):\n","\t\tcount=count+1\n","\t\tconverged=1\n","\t\t#Gets the Values based on the current policy, initially a random policy\n","\t\tV=eval_policy(policy,rewards,possible_actions,n,p_action,lbda,num_iterations=-1)\n","\t\tfor i in range(n):\n","\t\t\tfor j in range(n):\n","\t\t\t\tif(i==0 and j==0):\n","\t\t\t\t\tcontinue\n","\t\t\t\tif(i==0 and j==n-1):\n","\t\t\t\t\tcontinue\n","\t\t\t\taction_rewards=[]\n","\t\t\t\tindex=np.argmax(policy[i][j][1])\n","\t\t\t\tbest_action=policy[i][j][0][index]\n","\t\t\t\tfor action in possible_actions[i][j]:\n","\t\t\t\t\t#Fill in here\n","\t\t\t\tfinal_action_rewards=[]\n","\t\t\t\tfor ii in range(len(action_rewards)):\n","\t\t\t\t\t#FILL in here\n","\t\t\t\tstate_policy=np.zeros((len(final_action_rewards)))\n","\t\t\t\tstate_policy[np.argmax(final_action_rewards)]=1\n","\t\t\t\t#Creates a new best action for the current set of values and assigns it to the policy if the policy hasn't changed it has converged\n","\t\t\t\tif(np.argmax(final_action_rewards)!=np.argmax(policy[i][j][1])):\n","\t\t\t\t\tconverged=0\n","\t\t\t\tpolicy[i][j][1]=state_policy\n","\t\tbottom_left.append(V[n-1][0])\n","\t\tbottom_right.append(V[n-1][n-1])\n","\treturn(policy,bottom_left,bottom_right)"],"execution_count":0,"outputs":[]}]}